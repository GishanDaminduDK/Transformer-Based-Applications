{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10190936,"sourceType":"datasetVersion","datasetId":6296429}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n# Path to Kaggle input file\ncsv_file_path = \"/kaggle/input/english-to-sinhala-dataset/converted_data.csv\"\n\n# Check if the file exists and is a CSV\nif csv_file_path.endswith(\".csv\"):\n    # Read and print the content of the CSV file\n    df = pd.read_csv(csv_file_path)\n    print(\"Dataset Preview:\")\n    print(df.head())  # Display the first few rows of the dataset\nelse:\n    print(f\"The provided path is not a CSV file: {csv_file_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:09:37.676635Z","iopub.execute_input":"2024-12-13T15:09:37.677043Z","iopub.status.idle":"2024-12-13T15:09:39.698830Z","shell.execute_reply.started":"2024-12-13T15:09:37.676996Z","shell.execute_reply":"2024-12-13T15:09:39.697845Z"}},"outputs":[{"name":"stdout","text":"Dataset Preview:\n                                             Sinhala  \\\n0  c esc ma tete මෙය මගේ ප්‍රධාන අයිතියයි ඔබ ප්‍ර...   \n1  නෑ නෑ ඒක මගේ වරදක් අපිට හරි හමන් කැමරොන් කෙනෙක...   \n2  හැක් කිරීම සහ කට වහගෙන කෙළ ගසන කොටස නොවේ කරුණා...   \n3  හොඳයි, මම හිතුවා අපි උච්චාරණයෙන් පටන් ගනිමු එය...   \n4  ඔයා මගෙන් අහනවා ඒක හරිම ලස්සනයි ඔයාගේ නම මොකක්...   \n\n                                             English  \\\n0  c esc ma tete this is my headright see you re ...   \n1  no no it s my fault we didn t have a proper in...   \n2  not the hacking and gagging and spitting part ...   \n3  well i thought we d start with pronunciation i...   \n4  you re asking me out that s so cute what s you...   \n\n                                            Singlish  \n0  [Unkown] [Unkown] [Unkown] [Unkown] meya mage ...  \n1  ne ne eka mage waradak apita hari haman [Unkow...  \n2  hek kireema saha kata wahagena kela gasana kot...  \n3  hondayi mama hithuwa api uchcharanayen patan g...  \n4  oya magen ahanawa eka harima lassanayi oyage n...  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# Drop the third column (Singlish)\ndf = df.drop(df.columns[2], axis=1)  # Assuming the third column is the one to drop\n\n# Rename Sinhala and English columns to 'target_text'and 'source_text'\n# Adjust column indices or names based on the actual dataset structure\ndf.columns = ['target_text','source_text']\n\nprint(\"\\nUpdated Dataset Preview:\")\nprint(df.head())  # Display the updated dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:09:43.436103Z","iopub.execute_input":"2024-12-13T15:09:43.436933Z","iopub.status.idle":"2024-12-13T15:09:43.450065Z","shell.execute_reply.started":"2024-12-13T15:09:43.436884Z","shell.execute_reply":"2024-12-13T15:09:43.449263Z"}},"outputs":[{"name":"stdout","text":"\nUpdated Dataset Preview:\n                                         target_text  \\\n0  c esc ma tete මෙය මගේ ප්‍රධාන අයිතියයි ඔබ ප්‍ර...   \n1  නෑ නෑ ඒක මගේ වරදක් අපිට හරි හමන් කැමරොන් කෙනෙක...   \n2  හැක් කිරීම සහ කට වහගෙන කෙළ ගසන කොටස නොවේ කරුණා...   \n3  හොඳයි, මම හිතුවා අපි උච්චාරණයෙන් පටන් ගනිමු එය...   \n4  ඔයා මගෙන් අහනවා ඒක හරිම ලස්සනයි ඔයාගේ නම මොකක්...   \n\n                                         source_text  \n0  c esc ma tete this is my headright see you re ...  \n1  no no it s my fault we didn t have a proper in...  \n2  not the hacking and gagging and spitting part ...  \n3  well i thought we d start with pronunciation i...  \n4  you re asking me out that s so cute what s you...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data=df\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:09:47.088532Z","iopub.execute_input":"2024-12-13T15:09:47.089166Z","iopub.status.idle":"2024-12-13T15:09:47.102665Z","shell.execute_reply.started":"2024-12-13T15:09:47.089131Z","shell.execute_reply":"2024-12-13T15:09:47.101611Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                             target_text  \\\n0      c esc ma tete මෙය මගේ ප්‍රධාන අයිතියයි ඔබ ප්‍ර...   \n1      නෑ නෑ ඒක මගේ වරදක් අපිට හරි හමන් කැමරොන් කෙනෙක...   \n2      හැක් කිරීම සහ කට වහගෙන කෙළ ගසන කොටස නොවේ කරුණා...   \n3      හොඳයි, මම හිතුවා අපි උච්චාරණයෙන් පටන් ගනිමු එය...   \n4      ඔයා මගෙන් අහනවා ඒක හරිම ලස්සනයි ඔයාගේ නම මොකක්...   \n...                                                  ...   \n34464  ස්තූතියි නැපෝලියන් ෆ්‍රැන්සිස් ඔබ උණුසුම් ශීත ...   \n34465   සුභ සන්ධ්‍යාවක් සර් සුබ සන්ධ්‍යාවක් මේඩ්මොයිසෙල්   \n34466  කාලගුණය භයානකයි නේද සර් ඔව් එය මේ ශීත ඍතුවේ අප...   \n34467  ඔව්, එය මේ ශීත ඍතුවේ අප ගත කළ නරකම රාත්‍රියක් ...   \n34468  ඔබ මේ ආකාරයට දොරෙන් පිටත සිට ඇටකටු වලට සිසිල් ...   \n\n                                             source_text  \n0      c esc ma tete this is my headright see you re ...  \n1      no no it s my fault we didn t have a proper in...  \n2      not the hacking and gagging and spitting part ...  \n3      well i thought we d start with pronunciation i...  \n4      you re asking me out that s so cute what s you...  \n...                                                  ...  \n34464  thank you napoleon francis may i ask whether y...  \n34465        good evening sir good evening mademoiselle   \n34466  the weather is terrible isn t it sir yes it is...  \n34467  yes it is it must be one of the worst nights w...  \n34468  you must be chilled to the bone standing out o...  \n\n[34469 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target_text</th>\n      <th>source_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c esc ma tete මෙය මගේ ප්‍රධාන අයිතියයි ඔබ ප්‍ර...</td>\n      <td>c esc ma tete this is my headright see you re ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>නෑ නෑ ඒක මගේ වරදක් අපිට හරි හමන් කැමරොන් කෙනෙක...</td>\n      <td>no no it s my fault we didn t have a proper in...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>හැක් කිරීම සහ කට වහගෙන කෙළ ගසන කොටස නොවේ කරුණා...</td>\n      <td>not the hacking and gagging and spitting part ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>හොඳයි, මම හිතුවා අපි උච්චාරණයෙන් පටන් ගනිමු එය...</td>\n      <td>well i thought we d start with pronunciation i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ඔයා මගෙන් අහනවා ඒක හරිම ලස්සනයි ඔයාගේ නම මොකක්...</td>\n      <td>you re asking me out that s so cute what s you...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>34464</th>\n      <td>ස්තූතියි නැපෝලියන් ෆ්‍රැන්සිස් ඔබ උණුසුම් ශීත ...</td>\n      <td>thank you napoleon francis may i ask whether y...</td>\n    </tr>\n    <tr>\n      <th>34465</th>\n      <td>සුභ සන්ධ්‍යාවක් සර් සුබ සන්ධ්‍යාවක් මේඩ්මොයිසෙල්</td>\n      <td>good evening sir good evening mademoiselle</td>\n    </tr>\n    <tr>\n      <th>34466</th>\n      <td>කාලගුණය භයානකයි නේද සර් ඔව් එය මේ ශීත ඍතුවේ අප...</td>\n      <td>the weather is terrible isn t it sir yes it is...</td>\n    </tr>\n    <tr>\n      <th>34467</th>\n      <td>ඔව්, එය මේ ශීත ඍතුවේ අප ගත කළ නරකම රාත්‍රියක් ...</td>\n      <td>yes it is it must be one of the worst nights w...</td>\n    </tr>\n    <tr>\n      <th>34468</th>\n      <td>ඔබ මේ ආකාරයට දොරෙන් පිටත සිට ඇටකටු වලට සිසිල් ...</td>\n      <td>you must be chilled to the bone standing out o...</td>\n    </tr>\n  </tbody>\n</table>\n<p>34469 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data = data.sample(n=6000, random_state=42).reset_index(drop=True)\ndata.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:09:50.634704Z","iopub.execute_input":"2024-12-13T15:09:50.635079Z","iopub.status.idle":"2024-12-13T15:09:50.646302Z","shell.execute_reply.started":"2024-12-13T15:09:50.635048Z","shell.execute_reply":"2024-12-13T15:09:50.645304Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(6000, 2)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\ndata.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:09:53.437978Z","iopub.execute_input":"2024-12-13T15:09:53.438906Z","iopub.status.idle":"2024-12-13T15:09:53.449141Z","shell.execute_reply.started":"2024-12-13T15:09:53.438856Z","shell.execute_reply":"2024-12-13T15:09:53.448275Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"target_text    0\nsource_text    0\ndtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"\n# Convert all entries to strings\ndata[\"source_text\"] = data[\"source_text\"].astype(str)\ndata[\"target_text\"] = data[\"target_text\"].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:09:56.800194Z","iopub.execute_input":"2024-12-13T15:09:56.800529Z","iopub.status.idle":"2024-12-13T15:09:56.806796Z","shell.execute_reply.started":"2024-12-13T15:09:56.800501Z","shell.execute_reply":"2024-12-13T15:09:56.805854Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Split dataset into training and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n\n# Convert to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_data)\nval_dataset = Dataset.from_pandas(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:10:01.487181Z","iopub.execute_input":"2024-12-13T15:10:01.487515Z","iopub.status.idle":"2024-12-13T15:10:01.535812Z","shell.execute_reply.started":"2024-12-13T15:10:01.487488Z","shell.execute_reply":"2024-12-13T15:10:01.534967Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\ntrain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:10:06.090152Z","iopub.execute_input":"2024-12-13T15:10:06.090498Z","iopub.status.idle":"2024-12-13T15:10:06.096849Z","shell.execute_reply.started":"2024-12-13T15:10:06.090470Z","shell.execute_reply":"2024-12-13T15:10:06.095885Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['target_text', 'source_text', '__index_level_0__'],\n    num_rows: 5400\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load a tokenizer for the chosen model (e.g., mT5 or mBART)\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:10:10.510860Z","iopub.execute_input":"2024-12-13T15:10:10.511669Z","iopub.status.idle":"2024-12-13T15:10:19.014478Z","shell.execute_reply.started":"2024-12-13T15:10:10.511637Z","shell.execute_reply":"2024-12-13T15:10:19.013446Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a026ba4edfb4237ba707bf3c061ffea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"813966ee1a1e49c9b9eb3f915dce826c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c731d0ce88fd4c5bb01245868156950f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dff6703f7e242e2a46c53b0a3ae1470"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"input_max_len = max([len(tokenizer.encode(text)) for text in data['source_text']])\ninput_max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:12:08.627916Z","iopub.execute_input":"2024-12-13T14:12:08.628511Z","iopub.status.idle":"2024-12-13T14:12:09.156904Z","shell.execute_reply.started":"2024-12-13T14:12:08.628461Z","shell.execute_reply":"2024-12-13T14:12:09.155946Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"51"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"output_max_len = max([len(tokenizer.encode(text)) for text in data['target_text']])\noutput_max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:10:19.015830Z","iopub.execute_input":"2024-12-13T15:10:19.016295Z","iopub.status.idle":"2024-12-13T15:10:19.607892Z","shell.execute_reply.started":"2024-12-13T15:10:19.016267Z","shell.execute_reply":"2024-12-13T15:10:19.606968Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"163"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Tokenization function\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples['source_text'],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    labels = tokenizer(\n        examples['target_text'],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\nval_dataset = val_dataset.map(preprocess_function, batched=True)\n\n# Set format for PyTorch tensors\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:10:33.272888Z","iopub.execute_input":"2024-12-13T15:10:33.273295Z","iopub.status.idle":"2024-12-13T15:10:34.254748Z","shell.execute_reply.started":"2024-12-13T15:10:33.273263Z","shell.execute_reply":"2024-12-13T15:10:34.253832Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf6d8cde54c4313aea0f5f89eb7981f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b759dfa3c9c4b628d13ec7fdd6802ab"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:10:38.258283Z","iopub.execute_input":"2024-12-13T15:10:38.258631Z","iopub.status.idle":"2024-12-13T15:10:38.312428Z","shell.execute_reply.started":"2024-12-13T15:10:38.258603Z","shell.execute_reply":"2024-12-13T15:10:38.311592Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([250004,     17,    347, 143092,    903,   3687,     17,   3444,   1810,\n            111,    903,  33600, 108564,   5809,     17,   1884,  13505, 104130,\n           1884,  38074,      2,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'labels': tensor([250004,   4779,   1816,  99968, 179342,  27911,   6742,   1816,  13737,\n           2876,   9847,  29393,   5734,    722,  24335,  59487,      4,   6742,\n          70610,   1859,  13505, 104130,   1884,  38074,      2,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1,      1,      1,      1,      1,      1,      1,      1,\n              1,      1])}"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport os\n\n# Set paths for saving results and models in Kaggle\nresults_dir = \"/kaggle/working/results\"\nmodel_dir = \"/kaggle/working/my_trans_model\"\n\n# Create directories if they don't exist\nos.makedirs(results_dir, exist_ok=True)\nos.makedirs(model_dir, exist_ok=True)\n\n# Load pretrained tokenizer and model\nmodel_name = \"facebook/mbart-large-50\"  # Replace this if needed\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Set language-specific tokens for mBART (English to Sinhala)\ntokenizer.src_lang = \"en_XX\"  # Source language code (English)\ntokenizer.tgt_lang = \"si_LK\"  # Target language code (Sinhala)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=results_dir,               # Directory to save model and results\n    evaluation_strategy=\"epoch\",         # Evaluate model after every epoch\n    learning_rate=5e-5,                   # Learning rate\n    per_device_train_batch_size=8,        # Training batch size\n    per_device_eval_batch_size=8,         # Evaluation batch size\n    num_train_epochs=1,                   # Number of training epochs\n    weight_decay=0.01,                    # Weight decay for optimizer\n    save_strategy=\"epoch\",                # Save model checkpoint after every epoch\n    logging_dir=\"./logs\",                 # Directory for logging\n    predict_with_generate=True,           # Enable generation for evaluation\n    generation_max_length=128             # Max sequence length for generation\n)\n\n# Define the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,         # Replace with your training dataset\n    eval_dataset=val_dataset,           # Replace with your validation dataset\n    tokenizer=tokenizer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:11:14.678089Z","iopub.execute_input":"2024-12-13T15:11:14.678430Z","iopub.status.idle":"2024-12-13T15:11:45.538946Z","shell.execute_reply.started":"2024-12-13T15:11:14.678403Z","shell.execute_reply":"2024-12-13T15:11:45.538002Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6409ef2476ea44918049d80baf21a0e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065af4c23cc04ff7abd1a8795925723c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_24/995301876.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:12:39.618793Z","iopub.execute_input":"2024-12-13T15:12:39.619448Z","iopub.status.idle":"2024-12-13T15:22:00.520792Z","shell.execute_reply.started":"2024-12-13T15:12:39.619410Z","shell.execute_reply":"2024-12-13T15:22:00.519836Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241213_151301-tq7hxs88</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gishankalasinghe1999/huggingface/runs/tq7hxs88' target=\"_blank\">/kaggle/working/results</a></strong> to <a href='https://wandb.ai/gishankalasinghe1999/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gishankalasinghe1999/huggingface' target=\"_blank\">https://wandb.ai/gishankalasinghe1999/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gishankalasinghe1999/huggingface/runs/tq7hxs88' target=\"_blank\">https://wandb.ai/gishankalasinghe1999/huggingface/runs/tq7hxs88</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 08:55, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.032300</td>\n      <td>0.293146</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=675, training_loss=0.8430977432816117, metrics={'train_runtime': 560.5045, 'train_samples_per_second': 9.634, 'train_steps_per_second': 1.204, 'total_flos': 1462812593356800.0, 'train_loss': 0.8430977432816117, 'epoch': 1.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(model_dir)\ntokenizer.save_pretrained(model_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the directory if it doesn't exist\nos.makedirs(model_dir, exist_ok=True)\n\n# Save the fine-tuned model\ntrainer.save_model(model_dir)\n\n# Save the tokenizer explicitly\ntokenizer.save_pretrained(model_dir)\n\n\nloaded_model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\nloaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n\nprint(\"Model and tokenizer reloaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:23:34.359989Z","iopub.execute_input":"2024-12-13T15:23:34.360752Z","iopub.status.idle":"2024-12-13T15:23:46.043157Z","shell.execute_reply.started":"2024-12-13T15:23:34.360714Z","shell.execute_reply":"2024-12-13T15:23:46.042245Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer reloaded successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Create the directory if it doesn't exist\nos.makedirs(model_dir, exist_ok=True)\n\n# Save the fine-tuned model\ntrainer.save_model(model_dir)\n\n# Save the tokenizer explicitly\ntokenizer.save_pretrained(model_dir)\n\n\nloaded_model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\nloaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n\nprint(\"Model and tokenizer reloaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:03:04.536635Z","iopub.execute_input":"2024-12-13T15:03:04.537000Z","iopub.status.idle":"2024-12-13T15:03:15.862078Z","shell.execute_reply.started":"2024-12-13T15:03:04.536972Z","shell.execute_reply":"2024-12-13T15:03:15.860894Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer reloaded successfully!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def translate_text(text):\n    # Tokenize input\n    inputs = loaded_tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n    # Generate translation\n    outputs = loaded_model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n    # Decode the translation\n    translation = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return translation\n\n# Example usage\ntext_to_translate = \"You are supposed to do that task\"\ntranslated_text = translate_text(text_to_translate)\nprint(\"Translated text:\", translated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:24:49.680726Z","iopub.execute_input":"2024-12-13T15:24:49.681585Z","iopub.status.idle":"2024-12-13T15:24:51.389005Z","shell.execute_reply.started":"2024-12-13T15:24:49.681544Z","shell.execute_reply":"2024-12-13T15:24:51.388163Z"}},"outputs":[{"name":"stdout","text":"Translated text: ඔබ එම කාර්යය කිරීමට නියමිතය\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Example list of English sentences to translate\ntexts_to_translate = [\n    \"Children who are sixteen years old or younger are not allowed in the theater\",\n    \"She borrowed the book from him many years ago\",\n    \"She asked him to not quit his job because they needed the money\",\n    \"Tom would've liked to attend Mary's party, unfortunately, he couldn't\",\n    \"When you meet someone for the first time, be careful about your impressions\"\n]\n# Translate each sentence and print the result\nfor sentence in texts_to_translate:\n    translated_text = translate_text(sentence)\n    print(f\"Original: {sentence}\")\n    print(f\"Translated: {translated_text}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T15:25:46.477329Z","iopub.execute_input":"2024-12-13T15:25:46.477686Z","iopub.status.idle":"2024-12-13T15:26:04.323455Z","shell.execute_reply.started":"2024-12-13T15:25:46.477655Z","shell.execute_reply":"2024-12-13T15:26:04.322553Z"}},"outputs":[{"name":"stdout","text":"Original: Children who are sixteen years old or younger are not allowed in the theater\nTranslated: වයස අවුරුදු දහඅට අඩු දරුවන් වේදිකාවේ නොසිටීමට අවසර නැත\n\nOriginal: She borrowed the book from him many years ago\nTranslated: ඇය ඔහුගෙන් පොත් කිහිපයක් ලබා ගත්තා\n\nOriginal: She asked him to not quit his job because they needed the money\nTranslated: ඇය ඔහුට ඔහුගේ රැකියාවෙන් ඉවත් වීමට ප් රශ්නයක් ඇසුවා, ඔවුන්ට මුදල් අවශ් ය නිසාය\n\nOriginal: Tom would've liked to attend Mary's party, unfortunately, he couldn't\nTranslated: Tom Mary's party එකට සහභාගි වීමට කැමති විය, නමුත් වාසනාවකට මෙන්, ඔහුට නොහැකි විය\n\nOriginal: When you meet someone for the first time, be careful about your impressions\nTranslated: ඔබ පළමු වරට යමෙකු මුණගැසෙන විට ඔබේ හැඟීම් ගැන සැලකිලිමත් විය යුතුය\n\n","output_type":"stream"}],"execution_count":19}]}